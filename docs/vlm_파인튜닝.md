# VLM(Vision Language Model) íŒŒì¸íŠœë‹ ê°€ì´ë“œ

## 1. VLM ì•„í‚¤í…ì²˜ ê°œìš”

VLMì€ ì¼ë°˜ì ìœ¼ë¡œ ì„¸ ê°€ì§€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vision Encoder    â”‚  â† CLIP, SigLIP ë“± ì‚¬ì „í•™ìŠµëœ ë¹„ì „ ëª¨ë¸
â”‚   (ë¹„ì „ ì¸ì½”ë”)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Projector/Adapter  â”‚  â† Linear, MLP, Q-Former, Cross-Attention ë“±
â”‚   (í”„ë¡œì í„°/ì–´ëŒ‘í„°)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Language Model    â”‚  â† LLaMA, Qwen, Vicuna ë“± LLM
â”‚   (ì–¸ì–´ ëª¨ë¸)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. ë¹„ì „ ì¸ì½”ë” vs LLM ë¶„ë¦¬ í•™ìŠµì˜ ì›ë¦¬

### 2.1 ì™œ ë¶„ë¦¬í•´ì„œ í•™ìŠµí•˜ëŠ”ê°€?

VLM í•™ìŠµì—ì„œ ë¹„ì „ ì¸ì½”ë”ì™€ LLMì„ ë¶„ë¦¬í•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì€ **í‘œì¤€ì ì¸ ê´€í–‰**ì…ë‹ˆë‹¤:

| ì´ìœ  | ì„¤ëª… |
|------|------|
| **ì‚¬ì „í•™ìŠµ ì§€ì‹ ë³´ì¡´** | CLIP ê°™ì€ ë¹„ì „ ì¸ì½”ë”ëŠ” ì´ë¯¸ ìˆ˜ì–µ ê°œì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ í•™ìŠµë¨ |
| **ê³„ì‚° íš¨ìœ¨ì„±** | ì „ì²´ ëª¨ë¸ í•™ìŠµ ëŒ€ë¹„ í›¨ì”¬ ì ì€ ìì› í•„ìš” |
| **ì•ˆì •ì„±** | ë¹„ì „ ì¸ì½”ë” ë™ê²° ì‹œ í•™ìŠµì´ ë” ì•ˆì •ì  |
| **ëª¨ë“ˆí™”** | ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ êµì²´/ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥ |

### 2.2 ë¶„ë¦¬ í•™ìŠµì´ ê°€ëŠ¥í•œ ì´ìœ 

ë¹„ì „ ì¸ì½”ë”(íŠ¹íˆ CLIP ê³„ì—´)ëŠ” ì´ë¯¸ í…ìŠ¤íŠ¸ì™€ **cross-modal alignment**ê°€ ë˜ì–´ìˆì–´, í”„ë¡œì í„°ë§Œ í•™ìŠµí•´ë„ LLMì´ ì´ë¯¸ì§€ë¥¼ "ì´í•´"í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 3. VLM í•™ìŠµ ë‹¨ê³„ë³„ ì „ëµ

### 3.1 Stage 1: í”„ë¡œì í„° ì‚¬ì „í•™ìŠµ (Projector Pretraining)

```
Vision Encoder: â„ï¸ Frozen (ë™ê²°)
Projector:      ğŸ”¥ Training (í•™ìŠµ)
LLM:            â„ï¸ Frozen (ë™ê²°)
```

**ëª©ì **: ë¹„ì „ ì¸ì½”ë”ì˜ ì¶œë ¥ì„ LLMì˜ ì…ë ¥ ê³µê°„ì— ì •ë ¬(align)

**ë°ì´í„°**: ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ (ì˜ˆ: CC3M, LAION)

**íŠ¹ì§•**:
- ê°€ì¥ ì €ë ´í•œ í•™ìŠµ ë°©ë²•
- í”„ë¡œì í„°ë§Œ í•™ìŠµí•˜ë¯€ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì ìŒ
- ê¸°ë³¸ì ì¸ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥ ë¶€ì—¬

### 3.2 Stage 2: Visual Instruction Tuning

```
Vision Encoder: â„ï¸ Frozen (ë™ê²°)
Projector:      ğŸ”¥ Training (í•™ìŠµ)
LLM:            ğŸ”¥ Training (í•™ìŠµ) ë˜ëŠ” LoRA
```

**ëª©ì **: ëª¨ë¸ì´ ì‚¬ìš©ì ì§€ì‹œë¥¼ ë”°ë¥´ê³  ë³µì¡í•œ ì‹œê°ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë„ë¡ í•™ìŠµ

**ë°ì´í„°**: Visual instruction ë°ì´í„°ì…‹ (ì˜ˆ: LLaVA-Instruct, ShareGPT4V)

**íŠ¹ì§•**:
- LLMì˜ ëŠ¥ë ¥ì„ í™œìš©í•˜ë©´ì„œ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥ ê°•í™”
- LoRA ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

### 3.3 Stage 3 (ì„ íƒì ): End-to-End í•™ìŠµ

```
Vision Encoder: ğŸ”¥ Training (í•™ìŠµ)
Projector:      ğŸ”¥ Training (í•™ìŠµ)
LLM:            ğŸ”¥ Training (í•™ìŠµ)
```

**ëª©ì **: íŠ¹ìˆ˜í•œ ë„ë©”ì¸(ì˜ë£Œ, ë¬¸ì„œ, ìœ„ì„± ì´ë¯¸ì§€ ë“±)ì— ëŒ€í•œ ìµœì í™”

**ì£¼ì˜ì‚¬í•­**:
- ë§¤ìš° ë†’ì€ ê³„ì‚° ë¹„ìš©
- Catastrophic forgetting ìœ„í—˜
- ë‚®ì€ learning rate í•„ìˆ˜

## 4. ëª¨ë¸ë³„ í•™ìŠµ ì „ëµ ë¹„êµ

| ëª¨ë¸ | Stage 1 | Stage 2 | ë¹„ì „ ì¸ì½”ë” í•™ìŠµ ì—¬ë¶€ |
|------|---------|---------|-------------------|
| **LLaVA-1.5** | Projectorë§Œ | Projector + LLM | âŒ ë™ê²° |
| **LLaVA-NeXT** | Projectorë§Œ | Projector + LLM | âŒ ë™ê²° |
| **BLIP-2** | Q-Formerë§Œ | Q-Former + LLM | âŒ ë™ê²° |
| **Qwen2-VL** | ì „ì²´ | ì „ì²´ | âœ… í•™ìŠµ |
| **InternVL** | Projectorë§Œ | Projector + LLM | âœ… ì„ íƒì  í•™ìŠµ |
| **DeepSeek-VL** | Projectorë§Œ | Projector + LLM | âŒ ë™ê²° |
| **Fuyu-8B** | ì „ì²´ E2E | ì „ì²´ E2E | âœ… í•™ìŠµ (ì¸ì½”ë” ì—†ìŒ) |
| **KOSMOS-2** | ì „ì²´ E2E | ì „ì²´ E2E | âœ… í•™ìŠµ |

## 5. ì–´ë–¤ ìƒí™©ì—ì„œ ë¬´ì—‡ì„ í•™ìŠµì‹œí‚¤ëŠ”ê°€?

### 5.1 í”„ë¡œì í„°ë§Œ í•™ìŠµ (ê°€ì¥ íš¨ìœ¨ì )

**ì í•©í•œ ìƒí™©**:
- ì œí•œëœ GPU ìì› (24GB ì´í•˜)
- ì¼ë°˜ì ì¸ ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬
- ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

**ì˜ˆìƒ ê²°ê³¼**:
- ê¸°ë³¸ì ì¸ ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥
- ë³µì¡í•œ ì¶”ë¡ ì€ ì œí•œì 

### 5.2 í”„ë¡œì í„° + LLM(LoRA) í•™ìŠµ (ê¶Œì¥)

**ì í•©í•œ ìƒí™©**:
- ì¤‘ê°„ ìˆ˜ì¤€ì˜ GPU ìì› (40-80GB)
- ë„ë©”ì¸ íŠ¹í™” í•™ìŠµ (OCR, ì˜ë£Œ ë“±)
- ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜• í•„ìš”

**ì˜ˆìƒ ê²°ê³¼**:
- ìš°ìˆ˜í•œ instruction following
- ë„ë©”ì¸ íŠ¹í™” ì„±ëŠ¥ í–¥ìƒ

### 5.3 í”„ë¡œì í„° + LLM(Full) í•™ìŠµ

**ì í•©í•œ ìƒí™©**:
- ì¶©ë¶„í•œ GPU ìì›
- ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ë³´ìœ 
- ìµœê³  ì„±ëŠ¥ í•„ìš”

**ì˜ˆìƒ ê²°ê³¼**:
- ìµœì ì˜ ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥
- í•™ìŠµ ì‹œê°„ ì¦ê°€

### 5.4 ë¹„ì „ ì¸ì½”ë”ê¹Œì§€ í•™ìŠµ

**ì í•©í•œ ìƒí™©**:
- íŠ¹ìˆ˜í•œ ë„ë©”ì¸ ì´ë¯¸ì§€ (ì˜ë£Œ, ìœ„ì„±, ë¬¸ì„œ)
- ê¸°ì¡´ CLIP ì¸ì½”ë”ê°€ ì»¤ë²„í•˜ì§€ ëª»í•˜ëŠ” ì˜ì—­
- ë§¤ìš° ì„¸ë°€í•œ ì‹œê°ì  ì´í•´ í•„ìš” (OCR, ë¯¸ì„¸ í…ìŠ¤íŠ¸)

**ì£¼ì˜ì‚¬í•­**:
- í•™ìŠµ ë¶ˆì•ˆì • ìœ„í—˜ ë†’ìŒ
- ë§¤ìš° ë‚®ì€ learning rate ì‚¬ìš© (ì˜ˆ: 1e-6)
- ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œë§Œ ì§§ê²Œ í•™ìŠµ ê¶Œì¥
- ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”

## 6. ëª¨ë“  ëª¨ë¸ì—ì„œ ë¶„ë¦¬ í•™ìŠµì´ ê°€ëŠ¥í•œê°€?

### 6.1 ë¶„ë¦¬ í•™ìŠµ ì§€ì› ì—¬ë¶€

**ëŒ€ë¶€ë¶„ì˜ VLMì—ì„œ ì§€ì›ë¨**:

| í”„ë ˆì„ì›Œí¬ | ë¶„ë¦¬ í•™ìŠµ ì§€ì› | ì„¤ì • ë°©ë²• |
|-----------|--------------|----------|
| **LLaMA-Factory** | âœ… | `freeze_vision_tower`, `freeze_llm` |
| **lmms-finetune** | âœ… | Vision encoder LoRA ë³„ë„ ì„¤ì • |
| **Qwen-VL-Finetune** | âœ… | `--freeze_vision_tower`, `--freeze_llm` |
| **TRL (HuggingFace)** | âœ… | `model.vision_tower.requires_grad_(False)` |

### 6.2 ì˜ˆì™¸ ì¼€ì´ìŠ¤

ì¼ë¶€ ëª¨ë¸ì€ **ì•„í‚¤í…ì²˜ íŠ¹ì„±ìƒ** ë¶„ë¦¬ í•™ìŠµì´ ì œí•œë©ë‹ˆë‹¤:

- **Fuyu-8B**: ë³„ë„ì˜ ë¹„ì „ ì¸ì½”ë” ì—†ìŒ (ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ì§ì ‘ LLMì— ì…ë ¥)
- **EVEv2**: LLM ë‚´ë¶€ì— ë¹„ì „ ì¸ì‹ ê¸°ëŠ¥ í†µí•©
- **Native Multimodal Models**: ì²˜ìŒë¶€í„° ë©€í‹°ëª¨ë‹¬ë¡œ í•™ìŠµëœ ëª¨ë¸ (GPT-4V, Gemini)

### 6.3 í”„ë ˆì„ì›Œí¬ë³„ ì„¤ì • ì˜ˆì‹œ

**LLaMA-Factory**:
```yaml
# freeze ì„¤ì •
freeze_vision_tower: true
freeze_llm: false
lora_target: all  # LLMì— LoRA ì ìš©
```

**lmms-finetune**:
```bash
# ë¹„ì „ ì¸ì½”ë” ë™ê²°, LLMì— LoRA ì ìš©
python train.py \
    --freeze_vision_tower \
    --lora_enable \
    --lora_r 64
```

**PyTorch ì§ì ‘ ì„¤ì •**:
```python
# ë¹„ì „ ì¸ì½”ë” ë™ê²°
for param in model.vision_tower.parameters():
    param.requires_grad = False

# LLMì— LoRA ì ìš©
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=64,
    lora_alpha=128,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05
)
model = get_peft_model(model, lora_config)
```

## 7. ì‹¤ì „ ê¶Œì¥ ì‚¬í•­

### 7.1 OCR/ë¬¸ì„œ ì´í•´ íŒŒì¸íŠœë‹ ì‹œ

```
ì¶”ì²œ ì „ëµ: Stage 2 (Projector + LLM LoRA)
ë¹„ì „ ì¸ì½”ë”: ë™ê²° ìœ ì§€ (ì•ˆì •ì„±)
ì¶”ê°€ ê³ ë ¤: ê³ í•´ìƒë„ ì…ë ¥ ì§€ì› í™•ì¸
```

### 7.2 ë¹„ì „ ì¸ì½”ë” í•™ìŠµ ì‹œ ì£¼ì˜ì‚¬í•­

1. **Learning rateë¥¼ ë§¤ìš° ë‚®ê²Œ** (LLMì˜ 1/10 ~ 1/100)
2. **í•™ìŠµ í›„ë°˜ë¶€ì—ë§Œ** unfreeze
3. **ì¶©ë¶„í•œ ë°ì´í„°** í™•ë³´ (ìµœì†Œ ìˆ˜ë§Œ ìƒ˜í”Œ)
4. **Gradient checkpointing** í•„ìˆ˜

### 7.3 Catastrophic Forgetting ë°©ì§€

- **Replay ì „ëµ**: ê¸°ì¡´ ë°ì´í„° ì¼ë¶€ë¥¼ í•™ìŠµì— í¬í•¨
- **LoRA ì‚¬ìš©**: ì›ë³¸ ê°€ì¤‘ì¹˜ ë³´ì¡´
- **ë‚®ì€ epoch ìˆ˜**: 1-3 epoch ê¶Œì¥
- **Early stopping**: validation loss ëª¨ë‹ˆí„°ë§

## 8. VILA ì—°êµ¬ ê²°ê³¼ ìš”ì•½

NVIDIAì˜ VILA ë…¼ë¬¸ì—ì„œ ë°íŒ ì£¼ìš” ë°œê²¬:

| ë°œê²¬ | ì˜ë¯¸ |
|------|------|
| í”„ë¡œì í„°ë§Œ SFT â†’ ì„±ëŠ¥ ì €í•˜ | LLM í•™ìŠµì´ í•„ìˆ˜ì  |
| Pretraining ì‹œ LLM ë™ê²° â†’ 0-shotì€ ìœ ì§€, in-context learning ì €í•˜ | ì¼ë°˜í™” ëŠ¥ë ¥ì— ì˜í–¥ |
| ë‹¨ìˆœí•œ Linear projector > ë³µì¡í•œ Transformer projector | ë‹¨ìˆœí•¨ì´ ë•Œë¡œëŠ” ë” íš¨ê³¼ì  |

## 9. í˜„ì¬ í”„ë¡œì íŠ¸ êµ¬í˜„ (DeepSeek-OCR)

### 9.1 ì•„í‚¤í…ì²˜

```
Vision Encoder (SigLIP-SO400M-384)
        â”‚
        â–¼
   MLP Projector (2-layer)  â† multi_modal_projector
        â”‚
        â–¼
   LLM (DeepSeekMoE)
```

### 9.2 í•™ìŠµ ëª¨ë“œ

| ëª¨ë“œ | í•™ìŠµ ëŒ€ìƒ |
|------|----------|
| `vision` | Projector + Vision Encoder |
| `llm` | Projector + LLM |
| `both` | Projector + Vision Encoder + LLM |

**ëª¨ë“  ëª¨ë“œì—ì„œ ProjectorëŠ” ê¸°ë³¸ í¬í•¨ë©ë‹ˆë‹¤.**

### 9.3 ì„¤ì • ì˜ˆì‹œ (`config/train_config.yaml`)

```yaml
lora:
  # Projector: ëª¨ë“  ëª¨ë“œì—ì„œ ìë™ í¬í•¨
  projector_target_modules:
    - "multi_modal_projector"
    - "vision_embed_tokens"

  # Vision Encoder (mode: vision)
  vision_target_modules:
    - "qkv_proj"
    - "out_proj"
    - "fc1"
    - "fc2"

  # LLM (mode: llm)
  llm_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
```

### 9.4 ì‚¬ìš©ë²•

```bash
# LLM + Projector í•™ìŠµ (ê¶Œì¥)
TRAINING_MODE=llm uv run main.py train --dataset data.jsonl

# Vision Encoder + Projector í•™ìŠµ
TRAINING_MODE=vision uv run main.py train --dataset data.jsonl

# ì „ì²´ í•™ìŠµ
TRAINING_MODE=both uv run main.py train --dataset data.jsonl
```

---

## 10. ê²°ë¡ 

1. **ë¹„ì „ ì¸ì½”ë”ì™€ LLMì„ ë¶„ë¦¬í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê²ƒì€ í‘œì¤€ ê´€í–‰**ì…ë‹ˆë‹¤.
2. **ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë¹„ì „ ì¸ì½”ë”ëŠ” ë™ê²°**í•˜ê³  í”„ë¡œì í„° + LLMì„ í•™ìŠµí•©ë‹ˆë‹¤.
3. **ë„ë©”ì¸ íŠ¹í™” ì‹œê° ì´í•´ê°€ í•„ìš”í•œ ê²½ìš°**ì—ë§Œ ë¹„ì „ ì¸ì½”ë” í•™ìŠµì„ ê³ ë ¤í•©ë‹ˆë‹¤.
4. **ëª¨ë“  ì£¼ìš” VLM íŒŒì¸íŠœë‹ í”„ë ˆì„ì›Œí¬**ì—ì„œ ì´ëŸ¬í•œ ë¶„ë¦¬ í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.

---

## ì°¸ê³  ìë£Œ

- [Vision Language Models Explained - Hugging Face](https://huggingface.co/blog/vlms)
- [Design choices for Vision Language Models in 2024](https://huggingface.co/blog/gigant/vlm-design)
- [VILA: On Pre-training for Visual Language Models (CVPR 2024)](https://arxiv.org/html/2312.07533v3)
- [Fine-Tuning VLM with TRL - Hugging Face Cookbook](https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl)
- [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory)
- [Qwen-VL-Series-Finetune GitHub](https://github.com/2U1/Qwen-VL-Series-Finetune)
- [lmms-finetune GitHub](https://github.com/zjysteven/lmms-finetune)
- [VLM Training Process - Medium](https://medium.com/@hexiangnan/how-vision-language-models-are-trained-a-deep-dive-into-the-vlm-training-process-1ba1d8704bb0)
